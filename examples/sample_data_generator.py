#!/usr/bin/env python3
"""
ç¤ºä¾‹æ•°æ®ç”Ÿæˆå™¨
ç”¨äºç”Ÿæˆæµ‹è¯•ç”¨çš„ JSONL æ•°æ®æ–‡ä»¶
"""

import json
import random
import uuid
from datetime import datetime


def generate_sample_texts():
    """ç”Ÿæˆå¤šæ ·åŒ–çš„ç¤ºä¾‹æ–‡æœ¬å†…å®¹"""
    
    # ä¸åŒç±»å‹çš„ç¤ºä¾‹æ–‡æœ¬
    tech_texts = [
        "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ å·²ç»æˆä¸ºæ¨åŠ¨ç§‘æŠ€è¿›æ­¥çš„é‡è¦åŠ›é‡ã€‚",
        "äº‘è®¡ç®—ä¸ºä¼ä¸šæä¾›äº†çµæ´»ã€å¯æ‰©å±•çš„ITåŸºç¡€è®¾æ–½è§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—é™ä½äº†è¿è¥æˆæœ¬ã€‚",
        "åŒºå—é“¾æŠ€æœ¯å…·æœ‰å»ä¸­å¿ƒåŒ–ã€ä¸å¯ç¯¡æ”¹çš„ç‰¹ç‚¹ï¼Œåœ¨é‡‘èã€ä¾›åº”é“¾ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚",
        "ç‰©è”ç½‘è¿æ¥äº†æ•°åäº¿è®¾å¤‡ï¼Œä¸ºæ™ºæ…§åŸå¸‚å’Œå·¥ä¸š4.0çš„å‘å±•æä¾›äº†æŠ€æœ¯åŸºç¡€ã€‚",
        "5Gç½‘ç»œçš„éƒ¨ç½²å°†å¤§å¹…æå‡æ•°æ®ä¼ è¾“é€Ÿåº¦ï¼Œä¸ºè¿œç¨‹åŒ»ç–—ã€è‡ªåŠ¨é©¾é©¶ç­‰åº”ç”¨åˆ›é€ æ¡ä»¶ã€‚"
    ]
    
    science_texts = [
        "é‡å­è®¡ç®—åˆ©ç”¨é‡å­åŠ›å­¦åŸç†è¿›è¡Œä¿¡æ¯å¤„ç†ï¼Œæœ‰æœ›åœ¨ç‰¹å®šé—®é¢˜ä¸Šå®ç°æŒ‡æ•°çº§çš„æ€§èƒ½æå‡ã€‚",
        "åŸºå› ç¼–è¾‘æŠ€æœ¯CRISPRä¸ºæ²»ç–—é—ä¼ ç–¾ç—…å’Œæ”¹è‰¯å†œä½œç‰©å“ç§æä¾›äº†é©å‘½æ€§çš„å·¥å…·ã€‚",
        "å¯å†ç”Ÿèƒ½æºæŠ€æœ¯çš„å¿«é€Ÿå‘å±•æ­£åœ¨æ”¹å˜å…¨çƒèƒ½æºæ ¼å±€ï¼Œå¤ªé˜³èƒ½å’Œé£èƒ½æˆæœ¬æŒç»­ä¸‹é™ã€‚",
        "å¤ªç©ºæ¢ç´¢æŠ€æœ¯çš„è¿›æ­¥ä½¿å¾—ç«æ˜Ÿæ®–æ°‘å’Œå°è¡Œæ˜Ÿé‡‡çŸ¿ç­‰æ¦‚å¿µå˜å¾—æ›´åŠ ç°å®ã€‚",
        "çº³ç±³æŠ€æœ¯åœ¨ææ–™ç§‘å­¦ã€åŒ»å­¦å’Œç”µå­äº§å“ç­‰é¢†åŸŸå±•ç°å‡ºå·¨å¤§çš„åº”ç”¨æ½œåŠ›ã€‚"
    ]
    
    business_texts = [
        "æ•°å­—åŒ–è½¬å‹å·²æˆä¸ºä¼ä¸šä¿æŒç«äº‰åŠ›çš„å…³é”®ç­–ç•¥ï¼Œæ¶‰åŠä¸šåŠ¡æµç¨‹ã€å®¢æˆ·ä½“éªŒç­‰å¤šä¸ªæ–¹é¢ã€‚",
        "ç”µå­å•†åŠ¡å¹³å°çš„å…´èµ·æ”¹å˜äº†æ¶ˆè´¹è€…çš„è´­ç‰©ä¹ æƒ¯ï¼Œçº¿ä¸Šçº¿ä¸‹èåˆæˆä¸ºæ–°çš„å‘å±•è¶‹åŠ¿ã€‚",
        "æ•°æ®åˆ†æå’Œå•†ä¸šæ™ºèƒ½å·¥å…·å¸®åŠ©ä¼ä¸šä»æµ·é‡æ•°æ®ä¸­æå–æœ‰ä»·å€¼çš„æ´å¯Ÿå’Œå†³ç­–æ”¯æŒã€‚",
        "å¯æŒç»­å‘å±•å’ŒESGï¼ˆç¯å¢ƒã€ç¤¾ä¼šã€æ²»ç†ï¼‰æ ‡å‡†è¶Šæ¥è¶Šå—åˆ°æŠ•èµ„è€…å’Œæ¶ˆè´¹è€…çš„å…³æ³¨ã€‚",
        "è¿œç¨‹å·¥ä½œæ¨¡å¼çš„æ™®åŠæ”¹å˜äº†ä¼ ç»Ÿçš„åŠå…¬æ–‡åŒ–ï¼Œå¯¹ä¼ä¸šç®¡ç†æå‡ºäº†æ–°çš„æŒ‘æˆ˜ã€‚"
    ]
    
    culture_texts = [
        "ä¼ ç»Ÿæ–‡åŒ–ä¸ç°ä»£ç§‘æŠ€çš„èåˆåˆ›é€ äº†æ–°çš„è‰ºæœ¯è¡¨è¾¾å½¢å¼å’Œæ–‡åŒ–ä¼ æ’­æ–¹å¼ã€‚",
        "å…¨çƒåŒ–è¿›ç¨‹ä¸­çš„æ–‡åŒ–äº¤æµä¿ƒè¿›äº†ä¸åŒæ–‡æ˜ä¹‹é—´çš„ç†è§£å’ŒåŒ…å®¹ã€‚",
        "æ•°å­—åŒ–æ—¶ä»£çš„æ–‡åŒ–äº§ä¸šé¢ä¸´ç€æœºé‡å’ŒæŒ‘æˆ˜ï¼Œå†…å®¹åˆ›ä½œå’Œç‰ˆæƒä¿æŠ¤æˆä¸ºå…³é”®è®®é¢˜ã€‚",
        "æ•™è‚²æŠ€æœ¯çš„å‘å±•ä¸ºä¸ªæ€§åŒ–å­¦ä¹ å’Œç»ˆèº«æ•™è‚²æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚",
        "ç¤¾äº¤åª’ä½“å¹³å°æ”¹å˜äº†ä¿¡æ¯ä¼ æ’­å’Œç¤¾ä¼šäº’åŠ¨çš„æ–¹å¼ï¼Œå¯¹æ–‡åŒ–å½¢æˆäº§ç”Ÿæ·±è¿œå½±å“ã€‚"
    ]
    
    return tech_texts + science_texts + business_texts + culture_texts


def generate_sample_data(num_docs=1000, output_file="data/danbooru_training_data.jsonl"):
    """ç”Ÿæˆç¤ºä¾‹è®­ç»ƒæ•°æ®
    
    Args:
        num_docs: è¦ç”Ÿæˆçš„æ–‡æ¡£æ•°é‡
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    
    sample_texts = generate_sample_texts()
    
    print(f"æ­£åœ¨ç”Ÿæˆ {num_docs} ä¸ªç¤ºä¾‹æ–‡æ¡£...")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for i in range(num_docs):
            # éšæœºé€‰æ‹©åŸºç¡€æ–‡æœ¬
            base_text = random.choice(sample_texts)
            
            # æ·»åŠ ä¸€äº›å˜åŒ–
            variations = [
                f"åœ¨å½“ä»Šå¿«é€Ÿå‘å±•çš„ä¸–ç•Œä¸­ï¼Œ{base_text}",
                f"ç ”ç©¶è¡¨æ˜ï¼Œ{base_text}",
                f"ä¸“å®¶è®¤ä¸ºï¼Œ{base_text}",
                f"æ ¹æ®æœ€æ–°æŠ¥å‘Šï¼Œ{base_text}",
                f"éšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œ{base_text}",
                base_text,  # ä¿æŒåŸæ–‡
                f"{base_text}è¿™ä¸€è¶‹åŠ¿å€¼å¾—æˆ‘ä»¬æ·±å…¥æ€è€ƒå’Œå…³æ³¨ã€‚",
                f"{base_text}æœªæ¥çš„å‘å±•å‰æ™¯ä»¤äººæœŸå¾…ã€‚"
            ]
            
            final_text = random.choice(variations)
            
            # ç”Ÿæˆæ–‡æ¡£è®°å½•
            doc = {
                "id": f"doc_{i:06d}",
                "text": final_text,
                "source": f"sample_data/category_{i % 4}/document_{i}.txt"
            }
            
            # å†™å…¥ JSONL æ–‡ä»¶
            f.write(json.dumps(doc, ensure_ascii=False) + '\n')
    
    print(f"âœ… æˆåŠŸç”Ÿæˆ {num_docs} ä¸ªç¤ºä¾‹æ–‡æ¡£ï¼Œä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {get_file_size(output_file)}")
    
    # æ˜¾ç¤ºå‰å‡ è¡Œä½œä¸ºç¤ºä¾‹
    print("\nğŸ“‹ å‰3è¡Œç¤ºä¾‹:")
    with open(output_file, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= 3:
                break
            data = json.loads(line)
            print(f"{i+1}. ID: {data['id']}, Text: {data['text'][:50]}...")


def get_file_size(file_path):
    """è·å–æ–‡ä»¶å¤§å°çš„äººç±»å¯è¯»æ ¼å¼"""
    import os
    size_bytes = os.path.getsize(file_path)
    
    if size_bytes < 1024:
        return f"{size_bytes} B"
    elif size_bytes < 1024**2:
        return f"{size_bytes/1024:.1f} KB"
    elif size_bytes < 1024**3:
        return f"{size_bytes/(1024**2):.1f} MB"
    else:
        return f"{size_bytes/(1024**3):.1f} GB"


def validate_jsonl(file_path):
    """éªŒè¯ JSONL æ–‡ä»¶æ ¼å¼"""
    print(f"\nğŸ” éªŒè¯æ–‡ä»¶æ ¼å¼: {file_path}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            valid_lines = 0
            total_lines = 0
            
            for line_num, line in enumerate(f, 1):
                total_lines += 1
                line = line.strip()
                
                if not line:  # è·³è¿‡ç©ºè¡Œ
                    continue
                    
                try:
                    data = json.loads(line)
                    
                    # æ£€æŸ¥å¿…éœ€å­—æ®µ
                    if 'id' in data and 'text' in data and 'source' in data:
                        valid_lines += 1
                    else:
                        print(f"âš ï¸  ç¬¬ {line_num} è¡Œç¼ºå°‘å¿…éœ€å­—æ®µ: {data}")
                        
                except json.JSONDecodeError as e:
                    print(f"âŒ ç¬¬ {line_num} è¡Œ JSON æ ¼å¼é”™è¯¯: {e}")
            
            print(f"âœ… éªŒè¯å®Œæˆ: {valid_lines}/{total_lines} è¡Œæ ¼å¼æ­£ç¡®")
            return valid_lines == total_lines
            
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    import argparse
    import os
    
    parser = argparse.ArgumentParser(description="ç”Ÿæˆç¤ºä¾‹è®­ç»ƒæ•°æ®")
    parser.add_argument("--num-docs", "-n", type=int, default=1000,
                        help="è¦ç”Ÿæˆçš„æ–‡æ¡£æ•°é‡ (é»˜è®¤: 1000)")
    parser.add_argument("--output", "-o", type=str, default="data/danbooru_training_data.jsonl",
                        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: data/danbooru_training_data.jsonl)")
    parser.add_argument("--validate", "-v", action="store_true",
                        help="ç”ŸæˆåéªŒè¯æ–‡ä»¶æ ¼å¼")
    
    args = parser.parse_args()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(args.output), exist_ok=True)
    
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    generate_sample_data(args.num_docs, args.output)
    
    # éªŒè¯æ ¼å¼ï¼ˆå¦‚æœè¯·æ±‚ï¼‰
    if args.validate:
        validate_jsonl(args.output)
    
    print("\nğŸ‰ ç¤ºä¾‹æ•°æ®ç”Ÿæˆå®Œæˆï¼")
    print(f"ğŸ’¡ ä½¿ç”¨æ–¹æ³•: python main.py æˆ– bash scripts/run_vectorizer.sh")